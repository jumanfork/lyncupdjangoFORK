from celery import shared_task
# from chat.utils import create_chat_room
from asgiref.sync import async_to_sync
from django.contrib.auth import get_user_model
from channels.layers import get_channel_layer
from users.models import AppUser
import redis
from django.conf import settings
import logging
import pandas as pd
import os
import json


#import all the necessary functions for the matching algo
from matching.matching import match_in_cluster, run_batch_matching
from matching.distribute_rooms import distribute_rooms
from matching.queue_manager import ClusterQueueManager, UserEntry
from matching.build_clusters_annoy import create_similarity_matrix, create_clusters_and_annoy


logger = logging.getLogger(__name__)

#@shared_task decorator does not make the task available in all modules of project. 
#it only registers the task with Celery's task registry.
@shared_task
def build_clusters_annoy():
        # Get the directory of the current Python file
        base_dir = os.path.dirname(os.path.abspath(__file__))
        try:
            
            # Construct the full path to likes_df.csv
            likes_df = os.path.join(base_dir, "likes_df.csv")
            # Use the global CSV file path from settings
            likes_df = pd.read_csv(likes_df)
            # Process the DataFrame
            print("CSV File Loaded Successfully")
            


        except FileNotFoundError as e:
            print(f"File not found: {e}")
        except Exception as e:
            print(f"Error loading CSV files: {e}")


        create_clusters_and_annoy(likes_df, 5, None)

        



@shared_task
def run_matching_algo():

    #connect to Redis
    redis_client = redis.from_url(settings.REDIS_URL, decode_responses=True)

    # #get all user id's in queue in redis
    # try:
    #     membersIdSet = redis_client.smembers("queue")
    # except Exception as error:
    #     print(error)
    #     membersIdSet = set()

    # if not membersIdSet:
    #     print("queue is empty (from task.py)")
    #     return


    # #ensuring user_ids are int
    # try:
    #     user_ids = [int(member_id) for member_id in membersIdSet]
    # except ValueError as error:
    #     print(error)
    #     user_ids = []

    # #users is in QuerySet, not yet hit the database
    # users = AppUser.objects.filter(id__in=user_ids)

    # #convert to list, this is when Queryset hits the database
    # users = list(users)

    # print("users.count() before conditon < 2")


    #dummy data for testing
    user_ids = [1, 2, 3, 4, 5, 6, 7, 8, 10, 965, 993, 1884, 1934, 1977, 2104, 3782, 4391, 9, 23, 31, 48, 90, 108, 115, 120, 123]

    if len(user_ids) < 2:
        return

    logger.info("user_ids length")
    logger.info(len(user_ids))


    # #just some dummy thing, i'm not concerned about the matching algo for now, just will input dummy data
    # matched_groups = [{"room_id": 123, "user_ids": [3, 4]}]

    #now get the user_clusters json which is generated by create_clusters_and_annoy
    try:
        # Get the directory of the current Python file
        base_dir = os.path.dirname(os.path.abspath(__file__))
        #now get the user_clusters json
        user_clusters_path = os.path.join(base_dir, 'Annoy', 'user_clusters.json')

        #open and load the JSON file
        with open(user_clusters_path, 'r') as f:
            user_clusters = json.load(f)

        print("User cluster file loaded successfully")
    except FileNotFoundError as e:
        print(f"File not found: {e}")
    except Exception as e:
        print(f"Error loading user cluster files: {e}")

    queue_manager = ClusterQueueManager()

    for user_id in user_ids:
        queue_manager.add(user_clusters[str(user_id)], int(user_id))
    
    #run the batch matching algo
    grouped_users = run_batch_matching(queue_manager)
    #distribute grouped users to rooms
    matched_groups = distribute_rooms(grouped_users, redis_client)

    print(matched_groups)
    removed_ids = redis_client.smembers("rooms")
    print(removed_ids)
    if removed_ids:
        redis_client.srem("rooms", *removed_ids)

    # matched_groups = [
    #     {"room_id": 123, "user_ids": [1, 2, 3, 4]},
    #     {"room_id": 124, "user_ids": [5, 6, 7, 8]},
    #     ...
    # ]

    #get the channel layer
    # channel_layer = get_channel_layer()

    # success_matched_userIds = []

    # for group in matched_groups:
    #     room_id = group["room_id"]
    #     user_ids = group["user_ids"]

    #     for user_id in user_ids:
    #         try:
    #             async_to_sync(channel_layer.group_send)(
    #                 f'user_queue_{user_id}',
    #                 {
    #                     #Note: When Celery task sends a message via the channel layer, it doesn't need direct access to the consumer or its methods. Instead, it uses the channel layer as an intermediary to broadcast messages to any consumers that are subscribed to the relevant group.
    #                     "type": "send_room_id",
    #                     "room_id": room_id,
    #                 }
    #             )

    #             success_matched_userIds.append(user_id)

                
    #         except Exception as error:
    #             print(error)
    #     #srem command removes one or more members from a set.
    #     #*unpacks the elements of the collection, so instead of passing the collection as one argument, it passes each element of the collection as a separate argument
    #     #here is to remove the successfully matched users from the Redis queue.
    #     redis_client.srem("queue",*success_matched_userIds)
            







